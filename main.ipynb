{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d14130f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5f5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f84ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2136fc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0              1         0       3    1  22.0      1      0   7.2500         0\n",
       "1              2         1       1    0  38.0      1      0  71.2833         1\n",
       "2              3         1       3    0  26.0      0      0   7.9250         0\n",
       "3              4         1       1    0  35.0      1      0  53.1000         0\n",
       "4              5         0       3    1  35.0      0      0   8.0500         0\n",
       "..           ...       ...     ...  ...   ...    ...    ...      ...       ...\n",
       "885          886         0       3    0  39.0      0      5  29.1250         2\n",
       "886          887         0       2    1  27.0      0      0  13.0000         0\n",
       "887          888         1       1    0  19.0      0      0  30.0000         0\n",
       "889          890         1       1    1  26.0      0      0  30.0000         1\n",
       "890          891         0       3    1  32.0      0      0   7.7500         2\n",
       "\n",
       "[712 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "df.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "df.dropna(subset=['Age'], inplace=True)\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "\n",
    "df.dropna(subset=['Embarked'], inplace=True)\n",
    "df['Embarked'] = df['Embarked'].astype(int)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4ac6672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new table with only \"Survived\" as target\n",
    "Y = df['Survived'].to_numpy()\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86b8d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.    ,   3.    ,   1.    , ...,   0.    ,   7.25  ,   0.    ],\n",
       "       [  2.    ,   1.    ,   0.    , ...,   0.    ,  71.2833,   1.    ],\n",
       "       [  3.    ,   3.    ,   0.    , ...,   0.    ,   7.925 ,   0.    ],\n",
       "       ...,\n",
       "       [888.    ,   1.    ,   0.    , ...,   0.    ,  30.    ,   0.    ],\n",
       "       [890.    ,   1.    ,   1.    , ...,   0.    ,  30.    ,   1.    ],\n",
       "       [891.    ,   3.    ,   1.    , ...,   0.    ,   7.75  ,   2.    ]],\n",
       "      shape=(712, 8))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['Survived'], axis=1).to_numpy()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18c2b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, size_input):\n",
    "        self.bias = 0.01\n",
    "        self.weights = np.random.randn(size_input)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        a = self.sigmoid(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15db2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI:\n",
    "    def __init__(self, nb_epochs, learning_rate, model, X, Y):\n",
    "        # Hyperparameters\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def loss(self, a):\n",
    "        epsilon = 1e-10\n",
    "        a_clip = np.clip(a, epsilon, 1 - epsilon)\n",
    "        loss =  self.Y * np.log(a_clip) + (1 - self.Y) * np.log(1 - a_clip)\n",
    "        cost = -np.mean(loss)\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, a):\n",
    "        dZ = a - self.Y\n",
    "        m = self.Y.shape[0]\n",
    "\n",
    "        dW = (1/m) * np.dot(self.X.T, dZ)\n",
    "        db = (1/m) * np.sum(dZ)\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def gradient_descent(self, dW, db):\n",
    "        self.model.weights -= self.learning_rate * dW\n",
    "        self.model.bias    -= self.learning_rate * db\n",
    "\n",
    "    def train_epoch(self):\n",
    "\n",
    "        a = self.model.forward(self.X)\n",
    "        \n",
    "        loss = self.loss(a)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        \n",
    "        dW, db = self.backward_propagation(a)\n",
    "\n",
    "        self.gradient_descent(dW, db)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def train_ai(self):\n",
    "        for i in range(self.nb_epochs):\n",
    "            self.train_epoch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0e5c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.156401981095504\n",
      "Loss: 9.161426398415971\n",
      "Loss: 9.166491230951364\n",
      "Loss: 9.171591793523717\n",
      "Loss: 9.176113502182686\n",
      "Loss: 9.18067856251697\n",
      "Loss: 9.185331014536267\n",
      "Loss: 9.1901086258609\n",
      "Loss: 9.195045937051308\n",
      "Loss: 9.20015667422233\n",
      "Loss: 9.204876890888643\n",
      "Loss: 9.209160136722701\n",
      "Loss: 9.212859167231489\n",
      "Loss: 9.215444077857326\n",
      "Loss: 9.218121355072215\n",
      "Loss: 9.220897356138778\n",
      "Loss: 9.221914358843737\n",
      "Loss: 9.221143780010037\n",
      "Loss: 9.220576730527062\n",
      "Loss: 9.220433495128436\n",
      "Loss: 9.221072737527555\n",
      "Loss: 9.22249850617661\n",
      "Loss: 9.22440291303157\n",
      "Loss: 9.225828986513308\n",
      "Loss: 9.226976694925673\n",
      "Loss: 9.228605005165004\n",
      "Loss: 9.230752962652536\n",
      "Loss: 9.2333575977089\n",
      "Loss: 9.236310269802294\n",
      "Loss: 9.235271097580005\n",
      "Loss: 9.222816591291737\n",
      "Loss: 9.202310197713842\n",
      "Loss: 9.18229454958947\n",
      "Loss: 9.165466276874076\n",
      "Loss: 9.156569332195446\n",
      "Loss: 9.147401127564628\n",
      "Loss: 9.141771333138411\n",
      "Loss: 9.138194761750176\n",
      "Loss: 9.131648760654008\n",
      "Loss: 9.122488405110419\n",
      "Loss: 9.129249869757908\n",
      "Loss: 9.141412003744206\n",
      "Loss: 9.142309062315137\n",
      "Loss: 9.129591327326954\n",
      "Loss: 9.134738608365817\n",
      "Loss: 9.13818983302859\n",
      "Loss: 9.139736710938255\n",
      "Loss: 9.148923321382748\n",
      "Loss: 9.162082902145682\n",
      "Loss: 9.167707015919287\n",
      "Loss: 9.158061453409799\n",
      "Loss: 9.153118513497988\n",
      "Loss: 9.153966840600956\n",
      "Loss: 9.151620915725214\n",
      "Loss: 9.150860970260918\n",
      "Loss: 9.138697325952268\n",
      "Loss: 9.126004840968895\n",
      "Loss: 9.116534502266184\n",
      "Loss: 9.099035254050456\n",
      "Loss: 9.08570145617215\n",
      "Loss: 9.07242697992434\n",
      "Loss: 9.049284874798602\n",
      "Loss: 9.021203810927469\n",
      "Loss: 8.997973355169412\n",
      "Loss: 8.960682419942351\n",
      "Loss: 8.923052666781663\n",
      "Loss: 8.902304585249746\n",
      "Loss: 8.850890521685606\n",
      "Loss: 8.787569486798526\n",
      "Loss: 8.727202484133276\n",
      "Loss: 8.654494133830719\n",
      "Loss: 8.60908632597687\n",
      "Loss: 8.575111721584024\n",
      "Loss: 8.544297894559998\n",
      "Loss: 8.506455807274664\n",
      "Loss: 8.390756154420949\n",
      "Loss: 8.246849081349172\n",
      "Loss: 8.141327011832116\n",
      "Loss: 8.0995270889788\n",
      "Loss: 8.064087878853886\n",
      "Loss: 8.019231851145626\n",
      "Loss: 7.963867897659759\n",
      "Loss: 7.910244646816768\n",
      "Loss: 7.810138855768814\n",
      "Loss: 7.697166257859766\n",
      "Loss: 7.554480643114681\n",
      "Loss: 7.458857977118448\n",
      "Loss: 7.413806376186627\n",
      "Loss: 7.372114988376552\n",
      "Loss: 7.3054698854622515\n",
      "Loss: 7.235541559121677\n",
      "Loss: 7.154811424838662\n",
      "Loss: 7.093414065095009\n",
      "Loss: 7.056841701064883\n",
      "Loss: 7.0441688821221655\n",
      "Loss: 7.0441577826752075\n",
      "Loss: 7.043227553640335\n",
      "Loss: 7.045579147341149\n",
      "Loss: 7.047851099140286\n",
      "Loss: 7.04906910140746\n",
      "Loss: 7.049086009249751\n",
      "Loss: 7.048446070397544\n",
      "Loss: 7.047917868921173\n",
      "Loss: 7.047229496578716\n",
      "Loss: 7.04618913666839\n",
      "Loss: 7.044670952460357\n",
      "Loss: 7.0429087247112605\n",
      "Loss: 7.040912012636356\n",
      "Loss: 7.0387094102539765\n",
      "Loss: 7.036334766807473\n",
      "Loss: 7.033802513004954\n",
      "Loss: 7.031140128592007\n",
      "Loss: 7.028388257925116\n",
      "Loss: 7.025567031729674\n",
      "Loss: 7.022692438599019\n",
      "Loss: 7.01977704490782\n",
      "Loss: 7.016830602859317\n",
      "Loss: 7.013860658195664\n",
      "Loss: 7.010872996831089\n",
      "Loss: 7.00787206512753\n",
      "Loss: 7.004846447611432\n",
      "Loss: 7.001809921679867\n",
      "Loss: 6.998767891171852\n",
      "Loss: 6.9957218995913015\n",
      "Loss: 6.992673117598274\n",
      "Loss: 6.989622436440499\n",
      "Loss: 6.986570533905937\n",
      "Loss: 6.9835179212878105\n",
      "Loss: 6.980464985479644\n",
      "Loss: 6.977412024499492\n",
      "Loss: 6.974359259154804\n",
      "Loss: 6.971306858234027\n",
      "Loss: 6.968254952092995\n",
      "Loss: 6.965203636560654\n",
      "Loss: 6.962152986673979\n",
      "Loss: 6.959103053908807\n",
      "Loss: 6.956053886507821\n",
      "Loss: 6.953005510868167\n",
      "Loss: 6.949957954062835\n",
      "Loss: 6.946911231611258\n",
      "Loss: 6.9438388663261295\n",
      "Loss: 6.940764094049475\n",
      "Loss: 6.937690175134137\n",
      "Loss: 6.934617109220125\n",
      "Loss: 6.931544902780945\n",
      "Loss: 6.928473558451864\n",
      "Loss: 6.925403078628196\n",
      "Loss: 6.9223334666502785\n",
      "Loss: 6.919264723634559\n",
      "Loss: 6.916196847394266\n",
      "Loss: 6.913129838856781\n",
      "Loss: 6.910063699996214\n",
      "Loss: 6.906979223631145\n",
      "Loss: 6.90386583647759\n",
      "Loss: 6.900753326836153\n",
      "Loss: 6.897641701343475\n",
      "Loss: 6.89453095700458\n",
      "Loss: 6.891421095199231\n",
      "Loss: 6.888312112126049\n",
      "Loss: 6.885168428003762\n",
      "Loss: 6.881976686105528\n",
      "Loss: 6.878785824295624\n",
      "Loss: 6.875595836047995\n",
      "Loss: 6.872406725595536\n",
      "Loss: 6.869218489093974\n",
      "Loss: 6.866031129366163\n",
      "Loss: 6.862844644679115\n",
      "Loss: 6.859659034562172\n",
      "Loss: 6.856474296936666\n",
      "Loss: 6.853270914321418\n",
      "Loss: 6.850056046637323\n",
      "Loss: 6.846842035178425\n",
      "Loss: 6.843628885591306\n",
      "Loss: 6.840416593057833\n",
      "Loss: 6.837205162831885\n",
      "Loss: 6.8339945900857115\n",
      "Loss: 6.830784874080237\n",
      "Loss: 6.827576015335675\n",
      "Loss: 6.82436801455978\n",
      "Loss: 6.821160870632511\n",
      "Loss: 6.81795458398491\n",
      "Loss: 6.814749152854025\n",
      "Loss: 6.811544576494682\n",
      "Loss: 6.808340857073101\n",
      "Loss: 6.805137991304295\n",
      "Loss: 6.801935980364805\n",
      "Loss: 6.798734824033767\n",
      "Loss: 6.795534519990704\n",
      "Loss: 6.792335069711816\n",
      "Loss: 6.789136472389289\n",
      "Loss: 6.785938724699419\n",
      "Loss: 6.782741831856452\n",
      "Loss: 6.779545231289962\n",
      "Loss: 6.776305843423275\n",
      "Loss: 6.773067315215808\n",
      "Loss: 6.769829636725318\n",
      "Loss: 6.766592816460092\n",
      "Loss: 6.763356848804651\n",
      "Loss: 6.760121731157829\n",
      "Loss: 6.756887469904322\n",
      "Loss: 6.753654059520417\n",
      "Loss: 6.7504215027942145\n",
      "Loss: 6.747189794661818\n",
      "Loss: 6.743937527420082\n",
      "Loss: 6.740650697519883\n",
      "Loss: 6.737337077928198\n",
      "Loss: 6.734024326122016\n",
      "Loss: 6.73071243596401\n",
      "Loss: 6.727390670819869\n",
      "Loss: 6.7240331583035235\n",
      "Loss: 6.720631303506847\n",
      "Loss: 6.717230288262875\n",
      "Loss: 6.7138301172274435\n",
      "Loss: 6.7104307930138285\n",
      "Loss: 6.707032313602926\n",
      "Loss: 6.703634681232176\n",
      "Loss: 6.700237891001961\n",
      "Loss: 6.696841945534025\n",
      "Loss: 6.693432308533742\n",
      "Loss: 6.689987795700835\n",
      "Loss: 6.686544126696806\n",
      "Loss: 6.683100320631198\n",
      "Loss: 6.679611747164116\n",
      "Loss: 6.676124020385481\n",
      "Loss: 6.672637141808123\n",
      "Loss: 6.6691511117435\n",
      "Loss: 6.6656659347756095\n",
      "Loss: 6.662181604859878\n",
      "Loss: 6.658698123593362\n",
      "Loss: 6.65521549614293\n",
      "Loss: 6.651733718051735\n",
      "Loss: 6.648252787555706\n",
      "Loss: 6.644772709239384\n",
      "Loss: 6.641293484274073\n",
      "Loss: 6.637815108638293\n",
      "Loss: 6.634337583214569\n",
      "Loss: 6.630815888777591\n",
      "Loss: 6.6272666652559735\n",
      "Loss: 6.623718298097614\n",
      "Loss: 6.620133124125975\n",
      "Loss: 6.616496982185117\n",
      "Loss: 6.612861711143908\n",
      "Loss: 6.609227306630478\n",
      "Loss: 6.605593765908994\n",
      "Loss: 6.60196109587214\n",
      "Loss: 6.598329295203196\n",
      "Loss: 6.594698360088363\n",
      "Loss: 6.591068295842991\n",
      "Loss: 6.5874391019248115\n",
      "Loss: 6.583810778976324\n",
      "Loss: 6.5801833248849\n",
      "Loss: 6.576556739532412\n",
      "Loss: 6.572931029896285\n",
      "Loss: 6.569270907374948\n",
      "Loss: 6.565538360822485\n",
      "Loss: 6.561794527561646\n",
      "Loss: 6.558051581546252\n",
      "Loss: 6.5543095248214644\n",
      "Loss: 6.550568359735966\n",
      "Loss: 6.5468280851141785\n",
      "Loss: 6.543088703442301\n",
      "Loss: 6.539350217175498\n",
      "Loss: 6.535612622411973\n",
      "Loss: 6.531875921852637\n",
      "Loss: 6.528140119304795\n",
      "Loss: 6.524405208789812\n",
      "Loss: 6.520671201530798\n",
      "Loss: 6.51690668120563\n",
      "Loss: 6.513125787842052\n",
      "Loss: 6.509345802432455\n",
      "Loss: 6.505566719606676\n",
      "Loss: 6.501772780620732\n",
      "Loss: 6.497951123651324\n",
      "Loss: 6.494130376756706\n",
      "Loss: 6.4902935275399845\n",
      "Loss: 6.486436877180273\n",
      "Loss: 6.482581138280711\n",
      "Loss: 6.478726316089729\n",
      "Loss: 6.47487241740953\n",
      "Loss: 6.471019432896741\n",
      "Loss: 6.467167373165597\n",
      "Loss: 6.463316232863433\n",
      "Loss: 6.459466016120045\n",
      "Loss: 6.4555840940581115\n",
      "Loss: 6.451660060546284\n",
      "Loss: 6.447736954294727\n",
      "Loss: 6.443814779001966\n",
      "Loss: 6.439893529755692\n",
      "Loss: 6.43597321182193\n",
      "Loss: 6.432053826286364\n",
      "Loss: 6.428135374723582\n",
      "Loss: 6.424217852947935\n",
      "Loss: 6.420301271510073\n",
      "Loss: 6.4163856218360005\n",
      "Loss: 6.412470907674517\n",
      "Loss: 6.408557132401091\n",
      "Loss: 6.404644294244253\n",
      "Loss: 6.400732400440678\n",
      "Loss: 6.39682144571473\n",
      "Loss: 6.392911434754782\n",
      "Loss: 6.389002366598964\n",
      "Loss: 6.38509424311526\n",
      "Loss: 6.381187066435304\n",
      "Loss: 6.377280837974942\n",
      "Loss: 6.373375558476584\n",
      "Loss: 6.3694712294072025\n",
      "Loss: 6.365567853998411\n",
      "Loss: 6.3616465513557765\n",
      "Loss: 6.357701631965555\n",
      "Loss: 6.353757683756128\n",
      "Loss: 6.349814710975395\n",
      "Loss: 6.345872715364941\n",
      "Loss: 6.341931695834411\n",
      "Loss: 6.337991658016476\n",
      "Loss: 6.3340526019912655\n",
      "Loss: 6.330114531519834\n",
      "Loss: 6.3261774445072945\n",
      "Loss: 6.322241347357493\n",
      "Loss: 6.318306238040624\n",
      "Loss: 6.314372118705621\n",
      "Loss: 6.310438994649789\n",
      "Loss: 6.306506866113699\n",
      "Loss: 6.3025757340847735\n",
      "Loss: 6.298645602579948\n",
      "Loss: 6.294716470544006\n",
      "Loss: 6.290788343576279\n",
      "Loss: 6.2868612220619795\n",
      "Loss: 6.282920332740212\n",
      "Loss: 6.278955105794352\n",
      "Loss: 6.274990886451446\n",
      "Loss: 6.27102768328384\n",
      "Loss: 6.267065499340989\n",
      "Loss: 6.263065463174111\n",
      "Loss: 6.259054703957601\n",
      "Loss: 6.255044986839992\n",
      "Loss: 6.25103632005993\n",
      "Loss: 6.2470287073553505\n",
      "Loss: 6.243022147891514\n",
      "Loss: 6.23901664510434\n",
      "Loss: 6.2350121997870485\n",
      "Loss: 6.231008818956116\n",
      "Loss: 6.227006500644513\n",
      "Loss: 6.223005251663577\n",
      "Loss: 6.219005072393083\n",
      "Loss: 6.21500596548137\n",
      "Loss: 6.211007936353107\n",
      "Loss: 6.207010984408965\n",
      "Loss: 6.20301511418892\n",
      "Loss: 6.19902033002065\n",
      "Loss: 6.1950266322741685\n",
      "Loss: 6.191034025435669\n",
      "Loss: 6.186995285695083\n",
      "Loss: 6.182941686379082\n",
      "Loss: 6.178889212535112\n",
      "Loss: 6.174837868601201\n",
      "Loss: 6.170787657115855\n",
      "Loss: 6.16673858137013\n",
      "Loss: 6.162690645749004\n",
      "Loss: 6.15864385238731\n",
      "Loss: 6.154598205636471\n",
      "Loss: 6.1505363147537695\n",
      "Loss: 6.146453814802505\n",
      "Loss: 6.142372491525863\n",
      "Loss: 6.138292349738107\n",
      "Loss: 6.134213391254802\n",
      "Loss: 6.130096382499827\n",
      "Loss: 6.1259482721815495\n",
      "Loss: 6.12177247024079\n",
      "Loss: 6.1175860426796\n",
      "Loss: 6.113400805770146\n",
      "Loss: 6.109216750933573\n",
      "Loss: 6.1050338912427895\n",
      "Loss: 6.10085222718152\n",
      "Loss: 6.0966717570214985\n",
      "Loss: 6.092492496173224\n",
      "Loss: 6.088314435933698\n",
      "Loss: 6.084137591489766\n",
      "Loss: 6.079957804427228\n",
      "Loss: 6.075757060219872\n",
      "Loss: 6.0715575392900565\n",
      "Loss: 6.067359247758097\n",
      "Loss: 6.063162185882649\n",
      "Loss: 6.058966356592992\n",
      "Loss: 6.05477176903354\n",
      "Loss: 6.050578424961564\n",
      "Loss: 6.046386330619023\n",
      "Loss: 6.042195486590822\n",
      "Loss: 6.038005896210974\n",
      "Loss: 6.033802201662301\n",
      "Loss: 6.0295678018364285\n",
      "Loss: 6.0253346712583475\n",
      "Loss: 6.021102810447363\n",
      "Loss: 6.016872231426319\n",
      "Loss: 6.012642936574708\n",
      "Loss: 6.0084149275348135\n",
      "Loss: 6.004188214429411\n",
      "Loss: 5.999962798641965\n",
      "Loss: 5.995718248732377\n",
      "Loss: 5.991465197663905\n",
      "Loss: 5.987213459423823\n",
      "Loss: 5.982963042241769\n",
      "Loss: 5.978685660564574\n",
      "Loss: 5.974397307581466\n",
      "Loss: 5.97011030741921\n",
      "Loss: 5.96582466769786\n",
      "Loss: 5.961540392199121\n",
      "Loss: 5.957257489227273\n",
      "Loss: 5.952975959611562\n",
      "Loss: 5.948695808346591\n",
      "Loss: 5.944417048030512\n",
      "Loss: 5.9401281105364\n",
      "Loss: 5.935811551119621\n",
      "Loss: 5.9314964163714174\n",
      "Loss: 5.927182718293301\n",
      "Loss: 5.922870455684525\n",
      "Loss: 5.918559639057927\n",
      "Loss: 5.914250272621669\n",
      "Loss: 5.909942363447863\n",
      "Loss: 5.905635920116696\n",
      "Loss: 5.901330945022575\n",
      "Loss: 5.897009295569218\n",
      "Loss: 5.892661920537353\n",
      "Loss: 5.888316057518979\n",
      "Loss: 5.883971716754518\n",
      "Loss: 5.879628901029795\n",
      "Loss: 5.875287619442309\n",
      "Loss: 5.87094787538946\n",
      "Loss: 5.866609683231032\n",
      "Loss: 5.862254191518662\n",
      "Loss: 5.857875078441834\n",
      "Loss: 5.853459323025857\n",
      "Loss: 5.849011055085392\n",
      "Loss: 5.84455043892778\n",
      "Loss: 5.840019323381158\n",
      "Loss: 5.835380186398754\n",
      "Loss: 5.830729149282125\n",
      "Loss: 5.8260798063675905\n",
      "Loss: 5.821432152379852\n",
      "Loss: 5.816786204616665\n",
      "Loss: 5.812130447026343\n",
      "Loss: 5.807448556865768\n",
      "Loss: 5.8027683986102865\n",
      "Loss: 5.798089978353495\n",
      "Loss: 5.793413310685532\n",
      "Loss: 5.788738394015289\n",
      "Loss: 5.784065246346962\n",
      "Loss: 5.779389084250758\n",
      "Loss: 5.774673118908627\n",
      "Loss: 5.769958951441557\n",
      "Loss: 5.765246591841741\n",
      "Loss: 5.760536046969568\n",
      "Loss: 5.755827330551623\n",
      "Loss: 5.751120445243781\n",
      "Loss: 5.746415408279695\n",
      "Loss: 5.74171222702127\n",
      "Loss: 5.737010911314344\n",
      "Loss: 5.73231147111485\n",
      "Loss: 5.727613915768322\n",
      "Loss: 5.722918257355108\n",
      "Loss: 5.7182245072867435\n",
      "Loss: 5.713532673911989\n",
      "Loss: 5.708842767656899\n",
      "Loss: 5.7041548041306065\n",
      "Loss: 5.699468788845885\n",
      "Loss: 5.6947847359757535\n",
      "Loss: 5.69010265436073\n",
      "Loss: 5.685422560420083\n",
      "Loss: 5.680744457549525\n",
      "Loss: 5.676068365716672\n",
      "Loss: 5.671394295320307\n",
      "Loss: 5.666718168772408\n",
      "Loss: 5.661984792273079\n",
      "Loss: 5.6572534773333185\n",
      "Loss: 5.652462375614016\n",
      "Loss: 5.647627986451613\n",
      "Loss: 5.64276462581844\n",
      "Loss: 5.637863856539024\n",
      "Loss: 5.632965284070511\n",
      "Loss: 5.628068924433117\n",
      "Loss: 5.6231747954850615\n",
      "Loss: 5.618247284940066\n",
      "Loss: 5.6133099380621125\n",
      "Loss: 5.608374894609475\n",
      "Loss: 5.603442170222753\n",
      "Loss: 5.59851177991906\n",
      "Loss: 5.59356095509743\n",
      "Loss: 5.588575590991756\n",
      "Loss: 5.583592646506159\n",
      "Loss: 5.5786121339570505\n",
      "Loss: 5.573634066654316\n",
      "Loss: 5.568658466234966\n",
      "Loss: 5.563685344581737\n",
      "Loss: 5.558714717029214\n",
      "Loss: 5.553746601709714\n",
      "Loss: 5.54877752423483\n",
      "Loss: 5.543777535145347\n",
      "Loss: 5.538770326236399\n",
      "Loss: 5.53375650743697\n",
      "Loss: 5.52871050491832\n",
      "Loss: 5.523667158238203\n",
      "Loss: 5.518626477130419\n",
      "Loss: 5.513588484116709\n",
      "Loss: 5.508553195865633\n",
      "Loss: 5.503520630036766\n",
      "Loss: 5.498490800340785\n",
      "Loss: 5.493463727449642\n",
      "Loss: 5.488439425432497\n",
      "Loss: 5.483417919226783\n",
      "Loss: 5.478399221917479\n",
      "Loss: 5.473383347567724\n",
      "Loss: 5.468370324522856\n",
      "Loss: 5.463360162299704\n",
      "Loss: 5.458352880773972\n",
      "Loss: 5.453348498889953\n",
      "Loss: 5.448347040293155\n",
      "Loss: 5.443294347982822\n",
      "Loss: 5.438236419880312\n",
      "Loss: 5.4331814787539665\n",
      "Loss: 5.428129529497856\n",
      "Loss: 5.423080603808344\n",
      "Loss: 5.418034713915176\n",
      "Loss: 5.412991876708848\n",
      "Loss: 5.4079521187315835\n",
      "Loss: 5.402915451882502\n",
      "Loss: 5.39788189887393\n",
      "Loss: 5.392851476789592\n",
      "Loss: 5.387824206602588\n",
      "Loss: 5.38280010225255\n",
      "Loss: 5.377779192139444\n",
      "Loss: 5.372735632855083\n",
      "Loss: 5.367668791345981\n",
      "Loss: 5.362605201748266\n",
      "Loss: 5.357544883771775\n",
      "Loss: 5.352487854701659\n",
      "Loss: 5.3474341361174815\n",
      "Loss: 5.342383748839298\n",
      "Loss: 5.337336708534513\n",
      "Loss: 5.332293036671194\n",
      "Loss: 5.327252748335273\n",
      "Loss: 5.322215868428847\n",
      "Loss: 5.317182409291321\n",
      "Loss: 5.3121523972233815\n",
      "Loss: 5.307125843407623\n",
      "Loss: 5.302102771375008\n",
      "Loss: 5.297083197178305\n",
      "Loss: 5.2920655231457125\n",
      "Loss: 5.287025813053926\n",
      "Loss: 5.281989648222633\n",
      "Loss: 5.276954349681158\n",
      "Loss: 5.271875825624657\n",
      "Loss: 5.266800907066602\n",
      "Loss: 5.261712884885011\n",
      "Loss: 5.256565844247479\n",
      "Loss: 5.2513926194444736\n",
      "Loss: 5.246192697272567\n",
      "Loss: 5.240996495199128\n",
      "Loss: 5.235804023504514\n",
      "Loss: 5.230615313202869\n",
      "Loss: 5.225430370701926\n",
      "Loss: 5.2202492236233216\n",
      "Loss: 5.215071883107192\n",
      "Loss: 5.209898364446249\n",
      "Loss: 5.204728682580343\n",
      "Loss: 5.199533812513942\n",
      "Loss: 5.194279678616068\n",
      "Loss: 5.18902574175792\n",
      "Loss: 5.183775791445297\n",
      "Loss: 5.178529846498455\n",
      "Loss: 5.173287921383531\n",
      "Loss: 5.168050024688227\n",
      "Loss: 5.162816173100615\n",
      "Loss: 5.157586383799191\n",
      "Loss: 5.1523606675973195\n",
      "Loss: 5.1471390353679976\n",
      "Loss: 5.141852576806117\n",
      "Loss: 5.136550113275891\n",
      "Loss: 5.1312357092139\n",
      "Loss: 5.1259254507973155\n",
      "Loss: 5.120619360389816\n",
      "Loss: 5.115317437811747\n",
      "Loss: 5.110019706057884\n",
      "Loss: 5.104726167422905\n",
      "Loss: 5.099436835831675\n",
      "Loss: 5.094109432466782\n",
      "Loss: 5.088782435202377\n",
      "Loss: 5.083459708557182\n",
      "Loss: 5.0781412697382615\n",
      "Loss: 5.072827122452858\n",
      "Loss: 5.06751727488404\n",
      "Loss: 5.062211737976425\n",
      "Loss: 5.0569105178038285\n",
      "Loss: 5.051613621511717\n",
      "Loss: 5.046321056129448\n",
      "Loss: 5.04103282755707\n",
      "Loss: 5.0357489446214565\n",
      "Loss: 5.030469411175617\n",
      "Loss: 5.025194235469981\n",
      "Loss: 5.019923419762737\n",
      "Loss: 5.014656969676167\n",
      "Loss: 5.009394893265668\n",
      "Loss: 5.0041371929704175\n",
      "Loss: 4.998883871349301\n",
      "Loss: 4.99363493716579\n",
      "Loss: 4.988390391880128\n",
      "Loss: 4.983150236761405\n",
      "Loss: 4.977914475819751\n",
      "Loss: 4.97266415334609\n",
      "Loss: 4.967390186680006\n",
      "Loss: 4.962120650300038\n",
      "Loss: 4.956855548963267\n",
      "Loss: 4.9515948828407215\n",
      "Loss: 4.946331877347261\n",
      "Loss: 4.940975411306632\n",
      "Loss: 4.935602203843044\n",
      "Loss: 4.93023347647095\n",
      "Loss: 4.924869231734684\n",
      "Loss: 4.91950947054338\n",
      "Loss: 4.9141357049515255\n",
      "Loss: 4.90875403673016\n",
      "Loss: 4.90337685474129\n",
      "Loss: 4.897978635285609\n",
      "Loss: 4.892542774649271\n",
      "Loss: 4.887069346778022\n",
      "Loss: 4.881600454213808\n",
      "Loss: 4.876114220780661\n",
      "Loss: 4.870542824931605\n",
      "Loss: 4.864919128282827\n",
      "Loss: 4.859276681845732\n",
      "Loss: 4.85363883073309\n",
      "Loss: 4.848005579125895\n",
      "Loss: 4.8423769234396286\n",
      "Loss: 4.836752859976689\n",
      "Loss: 4.831133387247474\n",
      "Loss: 4.825518503471913\n",
      "Loss: 4.819847108630501\n",
      "Loss: 4.8141684697937475\n",
      "Loss: 4.808494443560776\n",
      "Loss: 4.80282502970456\n",
      "Loss: 4.7971602211658855\n",
      "Loss: 4.7915000221336035\n",
      "Loss: 4.7858444343612625\n",
      "Loss: 4.7801934557584485\n",
      "Loss: 4.7745470854351675\n",
      "Loss: 4.768905319441779\n",
      "Loss: 4.763268167878115\n",
      "Loss: 4.757635626509065\n",
      "Loss: 4.752007694109153\n",
      "Loss: 4.746384377055267\n",
      "Loss: 4.7407406697636905\n",
      "Loss: 4.735058894824457\n",
      "Loss: 4.729380558582369\n",
      "Loss: 4.72362329888125\n",
      "Loss: 4.717843688855977\n",
      "Loss: 4.712068766147577\n",
      "Loss: 4.70629852298418\n",
      "Loss: 4.700532971365478\n",
      "Loss: 4.694772113266383\n",
      "Loss: 4.689015949156107\n",
      "Loss: 4.683264487375778\n",
      "Loss: 4.677517729940047\n",
      "Loss: 4.671775685181043\n",
      "Loss: 4.666038356557549\n",
      "Loss: 4.660305746469866\n",
      "Loss: 4.654558473558199\n",
      "Loss: 4.648779376627937\n",
      "Loss: 4.643005031457584\n",
      "Loss: 4.637235443968729\n",
      "Loss: 4.63147061619831\n",
      "Loss: 4.625710562093229\n",
      "Loss: 4.619955284427939\n",
      "Loss: 4.614171580586972\n",
      "Loss: 4.608370214886528\n",
      "Loss: 4.60257367728429\n",
      "Loss: 4.596781974295294\n",
      "Loss: 4.590995118503782\n",
      "Loss: 4.585213119521735\n",
      "Loss: 4.5794359821519\n",
      "Loss: 4.57366372164993\n",
      "Loss: 4.567896345570602\n",
      "Loss: 4.562133867357069\n",
      "Loss: 4.556376296514061\n",
      "Loss: 4.5506236445115835\n",
      "Loss: 4.544875924653478\n",
      "Loss: 4.53913314821935\n",
      "Loss: 4.533395326334106\n",
      "Loss: 4.527662473662356\n",
      "Loss: 4.521914004118067\n",
      "Loss: 4.516156479204115\n",
      "Loss: 4.5104039650445005\n",
      "Loss: 4.504656477725642\n",
      "Loss: 4.498914031591527\n",
      "Loss: 4.4931766391799135\n",
      "Loss: 4.487444318744051\n",
      "Loss: 4.481717086454298\n",
      "Loss: 4.47599495485392\n",
      "Loss: 4.4702779411472795\n",
      "Loss: 4.4645660650108905\n",
      "Loss: 4.458859336540711\n",
      "Loss: 4.453157777246672\n",
      "Loss: 4.447378342195052\n",
      "Loss: 4.441595933489484\n",
      "Loss: 4.435818798177333\n",
      "Loss: 4.430046954093646\n",
      "Loss: 4.42428041915264\n",
      "Loss: 4.418519210785483\n",
      "Loss: 4.412763356346118\n",
      "Loss: 4.40699343475785\n",
      "Loss: 4.401202545054728\n",
      "Loss: 4.3954170745370975\n",
      "Loss: 4.389637052226316\n",
      "Loss: 4.3838624950246174\n",
      "Loss: 4.37809342184942\n",
      "Loss: 4.372329865136827\n",
      "Loss: 4.366571835202093\n",
      "Loss: 4.360819363608191\n",
      "Loss: 4.355072468898555\n",
      "Loss: 4.349331181325636\n",
      "Loss: 4.343595514586191\n",
      "Loss: 4.337865501847484\n",
      "Loss: 4.332141162343075\n",
      "Loss: 4.326422526074826\n",
      "Loss: 4.320709613719372\n",
      "Loss: 4.315002456253682\n",
      "Loss: 4.309301073850019\n",
      "Loss: 4.303605497011766\n",
      "Loss: 4.297915751412344\n",
      "Loss: 4.292231865764933\n",
      "Loss: 4.286553864070669\n",
      "Loss: 4.280881777566181\n",
      "Loss: 4.275215634449579\n",
      "Loss: 4.269555462119621\n",
      "Loss: 4.263901288408425\n",
      "Loss: 4.25825314449496\n",
      "Loss: 4.252593286661164\n",
      "Loss: 4.246937226592332\n",
      "Loss: 4.2412872863203415\n",
      "Loss: 4.235643494433883\n",
      "Loss: 4.229973245756404\n",
      "Loss: 4.224278423719669\n",
      "Loss: 4.218589831838542\n",
      "Loss: 4.212907507718274\n",
      "Loss: 4.207231474561967\n",
      "Loss: 4.201561767770012\n",
      "Loss: 4.195898421221285\n",
      "Loss: 4.190241462505388\n",
      "Loss: 4.1845909261247725\n",
      "Loss: 4.178946843822249\n",
      "Loss: 4.173309250459166\n",
      "Loss: 4.167678174256835\n",
      "Loss: 4.162053649009133\n",
      "Loss: 4.156435706520996\n",
      "Loss: 4.150824383515929\n",
      "Loss: 4.145219707748703\n",
      "Loss: 4.139621715540523\n",
      "Loss: 4.134030433165532\n",
      "Loss: 4.128445897500863\n",
      "Loss: 4.122868139367595\n",
      "Loss: 4.117297190835764\n",
      "Loss: 4.111733081210637\n",
      "Loss: 4.1061758454443495\n",
      "Loss: 4.100625513839296\n",
      "Loss: 4.095082114927051\n",
      "Loss: 4.089545680221212\n",
      "Loss: 4.084016240963908\n",
      "Loss: 4.078478219044985\n",
      "Loss: 4.072930185227175\n",
      "Loss: 4.0673892364857345\n",
      "Loss: 4.061855401135937\n",
      "Loss: 4.056328710397821\n",
      "Loss: 4.050809185103003\n",
      "Loss: 4.045296858372081\n",
      "Loss: 4.039791753503985\n",
      "Loss: 4.034293895455442\n",
      "Loss: 4.028797462148456\n",
      "Loss: 4.0232507645451765\n",
      "Loss: 4.017711442914546\n",
      "Loss: 4.012179520730659\n",
      "Loss: 4.006655020351396\n",
      "Loss: 4.001137965761015\n",
      "Loss: 3.9956283764463376\n",
      "Loss: 3.9901262774436397\n",
      "Loss: 3.98463168601791\n",
      "Loss: 3.9791446283732332\n",
      "Loss: 3.9736651123159294\n",
      "Loss: 3.968193165184843\n",
      "Loss: 3.9627288045899074\n",
      "Loss: 3.957272044400239\n",
      "Loss: 3.951822906546866\n",
      "Loss: 3.946381403247444\n",
      "Loss: 3.9409475532842166\n",
      "Loss: 3.9355213692086393\n",
      "Loss: 3.930102867174271\n",
      "Loss: 3.9246920634632145\n",
      "Loss: 3.9192889698190907\n",
      "Loss: 3.913893600371296\n",
      "Loss: 3.908505968241436\n",
      "Loss: 3.9031260851455536\n",
      "Loss: 3.8977539662052942\n",
      "Loss: 3.89238961985546\n",
      "Loss: 3.8870330592911744\n",
      "Loss: 3.8816842969641283\n",
      "Loss: 3.876343343109479\n",
      "Loss: 3.871010207661795\n",
      "Loss: 3.8656849021403694\n",
      "Loss: 3.86035251061918\n",
      "Loss: 3.8549982731322063\n",
      "Loss: 3.8496519191229193\n",
      "Loss: 3.8443134523102236\n",
      "Loss: 3.8389828911973547\n",
      "Loss: 3.833660236709406\n",
      "Loss: 3.82834550776727\n",
      "Loss: 3.823038707928859\n",
      "Loss: 3.817739849160397\n",
      "Loss: 3.8124489414185763\n",
      "Loss: 3.807134780840708\n",
      "Loss: 3.801795818511526\n",
      "Loss: 3.79646490867891\n",
      "Loss: 3.791142061388094\n",
      "Loss: 3.7858272808640656\n",
      "Loss: 3.780520582852712\n",
      "Loss: 3.775221976960826\n",
      "Loss: 3.7699314739242955\n",
      "Loss: 3.7646490838244877\n",
      "Loss: 3.75937481675109\n",
      "Loss: 3.7541086866057363\n",
      "Loss: 3.748850702703221\n",
      "Loss: 3.7436008776197083\n",
      "Loss: 3.7383592217891093\n",
      "Loss: 3.733125747002044\n",
      "Loss: 3.727900468987352\n",
      "Loss: 3.7226833946959204\n",
      "Loss: 3.717474541005627\n",
      "Loss: 3.7122739189385303\n",
      "Loss: 3.7070815431292745\n",
      "Loss: 3.7018974252823016\n",
      "Loss: 3.696721579462408\n",
      "Loss: 3.6915540193038376\n",
      "Loss: 3.68639476087562\n",
      "Loss: 3.6812438164584154\n",
      "Loss: 3.6761011987555494\n",
      "Loss: 3.6709669263957565\n",
      "Loss: 3.6658410116217817\n",
      "Loss: 3.6607234713301993\n",
      "Loss: 3.6556143189776056\n",
      "Loss: 3.650513572047163\n",
      "Loss: 3.6454212462640343\n",
      "Loss: 3.640337356626018\n",
      "Loss: 3.6352619211980937\n",
      "Loss: 3.6301551658743647\n",
      "Loss: 3.625045547744151\n",
      "Loss: 3.6199444556285783\n",
      "Loss: 3.6148519036101736\n",
      "Loss: 3.6097679090268042\n",
      "Loss: 3.6046924893844237\n",
      "Loss: 3.599625659818774\n",
      "Loss: 3.5945674403705974\n",
      "Loss: 3.5895178460300716\n",
      "Loss: 3.5844768956855755\n",
      "Loss: 3.5794446077306667\n",
      "Loss: 3.574420999512268\n",
      "Loss: 3.5694060914870165\n",
      "Loss: 3.5643999009867\n",
      "Loss: 3.559376623707227\n",
      "Loss: 3.554287367629999\n",
      "Loss: 3.5491790006652737\n",
      "Loss: 3.544058130222092\n",
      "Loss: 3.5389203718682642\n",
      "Loss: 3.533791545489009\n",
      "Loss: 3.528671672551385\n",
      "Loss: 3.5235607783084375\n",
      "Loss: 3.5184588728441386\n",
      "Loss: 3.51336598634277\n",
      "Loss: 3.508282134103145\n",
      "Loss: 3.5032073441230733\n",
      "Loss: 3.4981416314884335\n",
      "Loss: 3.4930850219433136\n",
      "Loss: 3.488037536054789\n",
      "Loss: 3.4829991983972484\n",
      "Loss: 3.477970028447475\n",
      "Loss: 3.472950053096414\n",
      "Loss: 3.467939290636051\n",
      "Loss: 3.462937768952941\n",
      "Loss: 3.4579455083228314\n",
      "Loss: 3.452962532334893\n",
      "Loss: 3.4479888683651523\n",
      "Loss: 3.44300877465872\n",
      "Loss: 3.4379726044758305\n",
      "Loss: 3.432945875897561\n",
      "Loss: 3.4279286127724715\n",
      "Loss: 3.422920834606436\n",
      "Loss: 3.4179225720342328\n",
      "Loss: 3.4129338477676896\n",
      "Loss: 3.4079546881966\n",
      "Loss: 3.4029851154966226\n",
      "Loss: 3.3980251556231784\n",
      "Loss: 3.393074833402794\n",
      "Loss: 3.3881341749940725\n",
      "Loss: 3.3832032065552933\n",
      "Loss: 3.3782819484600384\n",
      "Loss: 3.373303591564573\n",
      "Loss: 3.3683287140505933\n",
      "Loss: 3.3633636666470466\n",
      "Loss: 3.358408470685809\n",
      "Loss: 3.3534631586279677\n",
      "Loss: 3.3485277490356493\n",
      "Loss: 3.343602268855347\n",
      "Loss: 3.3386867414605823\n",
      "Loss: 3.333781186489103\n",
      "Loss: 3.3288856321034594\n",
      "Loss: 3.3239834990441017\n",
      "Loss: 3.3190607849113487\n",
      "Loss: 3.314148109214041\n",
      "Loss: 3.309221652660914\n",
      "Loss: 3.3042363698823607\n",
      "Loss: 3.2992612769110443\n",
      "Loss: 3.2942963915604135\n",
      "Loss: 3.2893417314385642\n",
      "Loss: 3.284397317298271\n",
      "Loss: 3.2794631724845513\n",
      "Loss: 3.2745393102263263\n",
      "Loss: 3.269548557637839\n",
      "Loss: 3.26456264188766\n",
      "Loss: 3.259587127635707\n",
      "Loss: 3.254622037434452\n",
      "Loss: 3.2496673762992527\n",
      "Loss: 3.2447231696654995\n",
      "Loss: 3.239741165569331\n",
      "Loss: 3.2347681524290546\n",
      "Loss: 3.229805595789409\n",
      "Loss: 3.224853507049648\n",
      "Loss: 3.2199119010061237\n",
      "Loss: 3.2149637179717305\n",
      "Loss: 3.209998947243768\n",
      "Loss: 3.2050446507605526\n",
      "Loss: 3.200100834690216\n",
      "Loss: 3.195167501981406\n",
      "Loss: 3.1901803486411526\n",
      "Loss: 3.185200351642672\n",
      "Loss: 3.180230897560061\n",
      "Loss: 3.1752719808655354\n",
      "Loss: 3.1703236122108467\n",
      "Loss: 3.165385793752871\n",
      "Loss: 3.160458533428329\n",
      "Loss: 3.1555418240413404\n",
      "Loss: 3.150635676736906\n",
      "Loss: 3.1457400860293205\n",
      "Loss: 3.140855053177312\n",
      "Loss: 3.135959423983883\n",
      "Loss: 3.1310087320038327\n",
      "Loss: 3.126068665416821\n",
      "Loss: 3.121139222150891\n",
      "Loss: 3.1162203995305693\n",
      "Loss: 3.111312194836932\n",
      "Loss: 3.1064146070888228\n",
      "Loss: 3.1015276243182273\n",
      "Loss: 3.0966512516806075\n",
      "Loss: 3.0917854748671414\n",
      "Loss: 3.0869302931040847\n",
      "Loss: 3.082085698296368\n",
      "Loss: 3.077251686240686\n",
      "Loss: 3.0724282472355644\n",
      "Loss: 3.067539890963523\n",
      "Loss: 3.0626355328489825\n",
      "Loss: 3.0577418344026834\n",
      "Loss: 3.05285879022077\n",
      "Loss: 3.0479863874541393\n",
      "Loss: 3.043124623370074\n",
      "Loss: 3.038273485518884\n",
      "Loss: 3.0334329660935735\n",
      "Loss: 3.0286030590963087\n",
      "Loss: 3.0236970865526995\n",
      "Loss: 3.0187357032131\n",
      "Loss: 3.013784999094815\n",
      "Loss: 3.0088284919444668\n",
      "Loss: 3.0038420861969795\n",
      "Loss: 2.9988663610993846\n",
      "Loss: 2.993901319109628\n",
      "Loss: 2.9889469386566407\n",
      "Loss: 2.9840032238588634\n",
      "Loss: 2.9790701610660797\n",
      "Loss: 2.97414774109278\n",
      "Loss: 2.9692359587962858\n",
      "Loss: 2.9643348014326474\n",
      "Loss: 2.9594442690677476\n",
      "Loss: 2.9545643497260685\n",
      "Loss: 2.949695037380283\n",
      "Loss: 2.9448363250956766\n",
      "Loss: 2.939988206028944\n",
      "Loss: 2.9351506730315124\n",
      "Loss: 2.930323721258015\n",
      "Loss: 2.925507342090571\n",
      "Loss: 2.9207015348699406\n",
      "Loss: 2.9159062903967166\n",
      "Loss: 2.9110735434654713\n",
      "Loss: 2.9062444917667283\n",
      "Loss: 2.901426016587016\n",
      "Loss: 2.8966181158377884\n",
      "Loss: 2.891820782408168\n",
      "Loss: 2.887034013386489\n",
      "Loss: 2.8822578076186836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11989/1968698385.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "my_perceptron = Perceptron(size_input=X.shape[1])\n",
    "\n",
    "ai = AI(nb_epochs=1000, learning_rate=0.0001, model=my_perceptron, X=X, Y=Y)\n",
    "\n",
    "ai.train_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013dabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
